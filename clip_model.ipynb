{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTlDbeHoabAk"
      },
      "source": [
        "## Housekeeping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvykqHDza5DV",
        "outputId": "0daaabd3-7912-48c0-f3ac-d6d91d799b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (0.15.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement cudatoolkit (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for cudatoolkit\u001b[0m\n",
            "Requirement already satisfied: ftfy in /opt/homebrew/lib/python3.11/site-packages (6.2.0)\n",
            "Requirement already satisfied: regex in /opt/homebrew/lib/python3.11/site-packages (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/homebrew/lib/python3.11/site-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/y2/_3pq15pd08556v211gq9jslr0000gn/T/pip-req-build-2d4u13x7\n",
            "  Running command git clone --filter=blob:none -q https://github.com/openai/CLIP.git /private/var/folders/y2/_3pq15pd08556v211gq9jslr0000gn/T/pip-req-build-2d4u13x7\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: ftfy in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (2.0.1)\n",
            "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (0.15.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/homebrew/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (4.9.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (3.12.4)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.28.2)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.24.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from torchvision->clip==1.0) (9.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: focal_loss_torch in /opt/homebrew/lib/python3.11/site-packages (0.1.2)\n",
            "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (from focal_loss_torch) (2.0.1)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from focal_loss_torch) (1.24.2)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch->focal_loss_torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch->focal_loss_torch) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch->focal_loss_torch) (4.9.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch->focal_loss_torch) (3.12.4)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch->focal_loss_torch) (1.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch->focal_loss_torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch->focal_loss_torch) (1.3.0)\n",
            "Requirement already satisfied: kaggle in /opt/homebrew/lib/python3.11/site-packages (1.6.12)\n",
            "Requirement already satisfied: python-slugify in /opt/homebrew/lib/python3.11/site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from kaggle) (2.28.2)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /opt/homebrew/lib/python3.11/site-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.10 in /opt/homebrew/lib/python3.11/site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: bleach in /opt/homebrew/lib/python3.11/site-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: urllib3 in /opt/homebrew/lib/python3.11/site-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil in /Users/sugardady/Library/Python/3.11/lib/python/site-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: webencodings in /opt/homebrew/lib/python3.11/site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /opt/homebrew/lib/python3.11/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->kaggle) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->kaggle) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "#!pip install pytorch torchvision cudatoolkit\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install focal_loss_torch\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "igfjsE9cabAl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from io import StringIO\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import BCELoss\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Bkqq1oabAn"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw91tE22abAn",
        "outputId": "d0580aad-6abd-4663-ed4d-5cc19b022eec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImageID</th>\n",
              "      <th>Labels</th>\n",
              "      <th>Caption</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5444</th>\n",
              "      <td>5444.jpg</td>\n",
              "      <td>7</td>\n",
              "      <td>A yellow train traveling down train tracks in ...</td>\n",
              "      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16727</th>\n",
              "      <td>16727.jpg</td>\n",
              "      <td>1 3 4</td>\n",
              "      <td>A man in yellow vest riding motorcycle in street.</td>\n",
              "      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26146</th>\n",
              "      <td>26146.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>A woman with nice breast holding an upside dow...</td>\n",
              "      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2183</th>\n",
              "      <td>2183.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>A woman is skiing down a snowy slope with moun...</td>\n",
              "      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29199</th>\n",
              "      <td>29199.jpg</td>\n",
              "      <td>11</td>\n",
              "      <td>A fire hydrant on someone's lawn in the grass ...</td>\n",
              "      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ImageID Labels                                            Caption  \\\n",
              "5444    5444.jpg      7  A yellow train traveling down train tracks in ...   \n",
              "16727  16727.jpg  1 3 4  A man in yellow vest riding motorcycle in street.   \n",
              "26146  26146.jpg      1  A woman with nice breast holding an upside dow...   \n",
              "2183    2183.jpg      1  A woman is skiing down a snowy slope with moun...   \n",
              "29199  29199.jpg     11  A fire hydrant on someone's lawn in the grass ...   \n",
              "\n",
              "                                              image_path  \n",
              "5444   /Users/sugardady/Documents/USYD/2024S1/COMP532...  \n",
              "16727  /Users/sugardady/Documents/USYD/2024S1/COMP532...  \n",
              "26146  /Users/sugardady/Documents/USYD/2024S1/COMP532...  \n",
              "2183   /Users/sugardady/Documents/USYD/2024S1/COMP532...  \n",
              "29199  /Users/sugardady/Documents/USYD/2024S1/COMP532...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DIR = '/Users/sugardady/Documents/USYD/2024S1/COMP5329/A2_due_17_may/COMP5329S1A2Dataset'\n",
        "TRAIN_FILENAME = os.path.join(DIR, \"train.csv\")\n",
        "TEST_FILENAME = os.path.join(DIR, \"test.csv\")\n",
        "IMAGES_DIR = os.path.join(DIR, \"data\")\n",
        "with open(TRAIN_FILENAME) as file:\n",
        "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
        "    data_df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
        "\n",
        "with open(TEST_FILENAME) as file:\n",
        "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
        "    test_df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
        "\n",
        "\n",
        "# Read and preprocess data\n",
        "data_df['image_path'] = data_df['ImageID'].apply(lambda x: os.path.join(IMAGES_DIR, x))\n",
        "test_df['image_path'] = test_df['ImageID'].apply(lambda x: os.path.join(IMAGES_DIR, x))\n",
        "# Split data into training and validation sets\n",
        "train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=5329)\n",
        "train_captions = train_df['Caption'].values\n",
        "val_captions = val_df['Caption'].values\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7Uf3be0abAn",
        "outputId": "270c93b9-0654-45b6-9963-c29db7382b88"
      },
      "outputs": [],
      "source": [
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z6MWH1GoabAn"
      },
      "outputs": [],
      "source": [
        "def preprocess_images(image_paths):\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        image = Image.open(path).convert(\"RGB\")  # Ensure images are in RGB\n",
        "        image = preprocess(image)  # Apply CLIP's preprocessing\n",
        "        images.append(image.unsqueeze(0))  # Add batch dimension\n",
        "    return torch.cat(images).to(device)  # Concatenate all images into a single tensor\n",
        "\n",
        "def tokenize_captions(captions):\n",
        "    return clip.tokenize(captions).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OffwgVPsabAo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extract image paths and captions from the DataFrame\n",
        "image_paths = train_df['image_path'].tolist()\n",
        "captions = train_df['Caption'].tolist()\n",
        "\n",
        "# Preprocess images and tokenize captions\n",
        "processed_images = preprocess_images(image_paths)\n",
        "tokenized_captions = tokenize_captions(captions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QXxOVAuabAo",
        "outputId": "3ec5c97a-042f-42bd-bf41-308113f056b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels: ['1' '10' '11' '13' '14' '15' '16' '17' '18' '19' '2' '3' '4' '5' '6' '7'\n",
            " '8' '9']\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [1 1 0 ... 0 1 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# Assume 'data_df' is your DataFrame and it has a 'Labels' column where labels are space-separated\n",
        "train_df['Labels'] = train_df['Labels'].apply(lambda x: x.split())\n",
        "val_df['Labels'] = val_df['Labels'].apply(lambda x: x.split())\n",
        "# Initialize the MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Fit and transform the labels to one-hot encoding\n",
        "train_labels_one_hot = mlb.fit_transform(train_df['Labels'])\n",
        "val_labels_one_hot = mlb.fit_transform(val_df['Labels'])\n",
        "# mlb.classes_ will give you the array of all labels that the binarizer has seen\n",
        "print(\"Labels:\", mlb.classes_)\n",
        "\n",
        "# Example output\n",
        "print(val_labels_one_hot)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N1PwoUMiabAo"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_labels_one_hot' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m tokenize \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mtokenize\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[1;32m     29\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m CustomDataset(\n\u001b[1;32m     30\u001b[0m     image_paths\u001b[38;5;241m=\u001b[39mtrain_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m     31\u001b[0m     captions\u001b[38;5;241m=\u001b[39mtrain_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCaption\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m---> 32\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_labels_one_hot\u001b[49m,\n\u001b[1;32m     33\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m     34\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39mtokenize\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m CustomDataset(\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m     image_paths\u001b[38;5;241m=\u001b[39mval_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39mtokenize\n\u001b[1;32m     45\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_labels_one_hot' is not defined"
          ]
        }
      ],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_paths, captions, labels, transform, tokenize):\n",
        "        self.image_paths = image_paths\n",
        "        self.captions = captions\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.tokenize = tokenize\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Image processing\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Caption processing\n",
        "        caption = self.tokenize([self.captions[idx]])\n",
        "\n",
        "        # Labels\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "\n",
        "        return image, caption.squeeze(0), label\n",
        "# Note: These should be functions that operate on a single item and return a tensor\n",
        "transform = preprocess\n",
        "tokenize = clip.tokenize\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_ds = CustomDataset(\n",
        "    image_paths=train_df['image_path'].tolist(),\n",
        "    captions=train_df['Caption'].tolist(),\n",
        "    labels=train_labels_one_hot,\n",
        "    transform=transform,\n",
        "    tokenize=tokenize\n",
        ")\n",
        "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "val_ds = CustomDataset(\n",
        "\n",
        "    image_paths=val_df['image_path'].tolist(),\n",
        "    captions=val_df['Caption'].tolist(),\n",
        "    labels=val_labels_one_hot,\n",
        "    transform=transform,\n",
        "    tokenize=tokenize\n",
        ")\n",
        "val_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzB_lYvxabAp",
        "outputId": "173bb357-24be-47c7-f216-41142722d2e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images: torch.Size([64, 3, 224, 224])\n",
            "Captions: torch.Size([64, 77])\n",
            "Labels: torch.Size([64, 18])\n"
          ]
        }
      ],
      "source": [
        "# Quick check to see if everything is loading correctly\n",
        "for images, captions, labels in train_dataloader:\n",
        "    print('Images:', images.shape)  # Expect: [batch_size, C, H, W]\n",
        "    print('Captions:', captions.shape)  # Expect: [batch_size, L]\n",
        "    print('Labels:', labels.shape)  # Expect: [batch_size, num_labels]\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtwMh6LNabAp"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wv795ChfabAp"
      },
      "outputs": [],
      "source": [
        "class MultiLabelCLIPClassifier(nn.Module):\n",
        "    def __init__(self, clip_model, num_labels):\n",
        "        super(MultiLabelCLIPClassifier, self).__init__()\n",
        "        self.clip_model = clip_model\n",
        "        self.fc = nn.Linear(512, num_labels)  # 512 is an example feature size, adjust based on CLIP's output\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        #self.float()\n",
        "\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        with torch.no_grad():\n",
        "            image_features = self.clip_model.encode_image(images)\n",
        "            text_features = self.clip_model.encode_text(captions)\n",
        "\n",
        "        # Combining features: simple addition or concatenation can be used depending on the task\n",
        "        features = (image_features + text_features) / 2\n",
        "        output = self.fc(features)\n",
        "        output = self.sigmoid(output)  # Sigmoid activation for multi-label classification\n",
        "        return output\n",
        "\n",
        "# Initialize the model\n",
        "num_class = 18\n",
        "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "model = MultiLabelCLIPClassifier(clip_model, num_class).to(device)  # 18 is the number of labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UHrNkTinabAp"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auGeJPqJabAp"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 30  # Set the number of epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8c3s_q_jv5W"
      },
      "source": [
        "### Model with Multi Label Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L955GRcjabAp",
        "outputId": "b2f4c48a-3bb6-46f0-9e2d-c25b5af18c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Train Loss: 0.1411, Val Loss: 0.1252\n",
            "Validation loss decreased (inf --> 0.125155).  Saving model ...\n",
            "Epoch [2/30], Train Loss: 0.1184, Val Loss: 0.1138\n",
            "Validation loss decreased (0.125155 --> 0.113775).  Saving model ...\n",
            "Epoch [3/30], Train Loss: 0.1115, Val Loss: 0.1096\n",
            "Validation loss decreased (0.113775 --> 0.109649).  Saving model ...\n",
            "Epoch [4/30], Train Loss: 0.1086, Val Loss: 0.1077\n",
            "Validation loss decreased (0.109649 --> 0.107692).  Saving model ...\n",
            "Epoch [5/30], Train Loss: 0.1071, Val Loss: 0.1066\n",
            "Validation loss decreased (0.107692 --> 0.106596).  Saving model ...\n",
            "Epoch [6/30], Train Loss: 0.1062, Val Loss: 0.1059\n",
            "Validation loss decreased (0.106596 --> 0.105910).  Saving model ...\n",
            "Epoch [7/30], Train Loss: 0.1057, Val Loss: 0.1055\n",
            "Validation loss decreased (0.105910 --> 0.105452).  Saving model ...\n",
            "Epoch [8/30], Train Loss: 0.1053, Val Loss: 0.1051\n",
            "Validation loss decreased (0.105452 --> 0.105128).  Saving model ...\n",
            "Epoch [9/30], Train Loss: 0.1050, Val Loss: 0.1049\n",
            "Validation loss decreased (0.105128 --> 0.104888).  Saving model ...\n",
            "Epoch [10/30], Train Loss: 0.1048, Val Loss: 0.1047\n",
            "Validation loss decreased (0.104888 --> 0.104704).  Saving model ...\n",
            "Epoch [11/30], Train Loss: 0.1046, Val Loss: 0.1046\n",
            "Validation loss decreased (0.104704 --> 0.104557).  Saving model ...\n",
            "Epoch [12/30], Train Loss: 0.1045, Val Loss: 0.1044\n",
            "Validation loss decreased (0.104557 --> 0.104426).  Saving model ...\n",
            "Epoch [13/30], Train Loss: 0.1044, Val Loss: 0.1043\n",
            "Validation loss decreased (0.104426 --> 0.104306).  Saving model ...\n",
            "Epoch [14/30], Train Loss: 0.1043, Val Loss: 0.1042\n",
            "Validation loss decreased (0.104306 --> 0.104197).  Saving model ...\n",
            "Epoch [15/30], Train Loss: 0.1042, Val Loss: 0.1041\n",
            "Validation loss decreased (0.104197 --> 0.104083).  Saving model ...\n",
            "Epoch [16/30], Train Loss: 0.1041, Val Loss: 0.1040\n",
            "Validation loss decreased (0.104083 --> 0.103993).  Saving model ...\n",
            "Epoch [17/30], Train Loss: 0.1040, Val Loss: 0.1039\n",
            "Validation loss decreased (0.103993 --> 0.103933).  Saving model ...\n",
            "Epoch [18/30], Train Loss: 0.1039, Val Loss: 0.1039\n",
            "Validation loss decreased (0.103933 --> 0.103883).  Saving model ...\n",
            "Epoch [19/30], Train Loss: 0.1039, Val Loss: 0.1038\n",
            "Validation loss decreased (0.103883 --> 0.103844).  Saving model ...\n",
            "Epoch [20/30], Train Loss: 0.1039, Val Loss: 0.1038\n",
            "Validation loss decreased (0.103844 --> 0.103812).  Saving model ...\n",
            "Epoch [21/30], Train Loss: 0.1038, Val Loss: 0.1038\n",
            "Validation loss decreased (0.103812 --> 0.103785).  Saving model ...\n",
            "Epoch [22/30], Train Loss: 0.1038, Val Loss: 0.1038\n",
            "Validation loss decreased (0.103785 --> 0.103759).  Saving model ...\n",
            "Epoch [23/30], Train Loss: 0.1038, Val Loss: 0.1037\n",
            "Validation loss decreased (0.103759 --> 0.103725).  Saving model ...\n",
            "Epoch [24/30], Train Loss: 0.1037, Val Loss: 0.1036\n",
            "Validation loss decreased (0.103725 --> 0.103622).  Saving model ...\n",
            "Epoch [25/30], Train Loss: 0.1036, Val Loss: 0.1035\n",
            "Validation loss decreased (0.103622 --> 0.103545).  Saving model ...\n",
            "Epoch [26/30], Train Loss: 0.1035, Val Loss: 0.1035\n",
            "Validation loss decreased (0.103545 --> 0.103489).  Saving model ...\n",
            "Epoch [27/30], Train Loss: 0.1035, Val Loss: 0.1034\n",
            "Validation loss decreased (0.103489 --> 0.103431).  Saving model ...\n",
            "Epoch [28/30], Train Loss: 0.1034, Val Loss: 0.1034\n",
            "Validation loss decreased (0.103431 --> 0.103401).  Saving model ...\n",
            "Epoch [29/30], Train Loss: 0.1034, Val Loss: 0.1034\n",
            "Validation loss decreased (0.103401 --> 0.103379).  Saving model ...\n",
            "Epoch [30/30], Train Loss: 0.1034, Val Loss: 0.1034\n",
            "Validation loss decreased (0.103379 --> 0.103363).  Saving model ...\n"
          ]
        }
      ],
      "source": [
        "class FocalLossMultiLabel(torch.nn.Module):\n",
        "    def __init__(self, gamma=2.0, alpha=0.25):\n",
        "        super(FocalLossMultiLabel, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
        "        probs = torch.sigmoid(logits)\n",
        "        p_t = targets * probs + (1 - targets) * (1 - probs)\n",
        "        fl_loss = self.alpha * (1 - p_t) ** self.gamma * bce_loss\n",
        "        return fl_loss.mean()\n",
        "\n",
        "# Define the criterion with Focal Loss\n",
        "criterion = FocalLossMultiLabel(gamma=0.7)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "#scheduler = CosineAnnealingLR(optimizer, T_max=100)  # Adjust T_max based on your training epochs\n",
        "\n",
        "# Initialize early stopping object\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, path='best_model_focal_adamw.pt')\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, captions, labels in train_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, captions)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Learning rate scheduling step\n",
        "    #scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():  # Use torch.no_grad() during validation\n",
        "        for images, captions, labels in val_dataloader:\n",
        "            images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "            outputs = model(images, captions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    val_loss /= len(val_dataloader)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Early stopping and model checkpointing\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoLQMM9wabAq"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMdmw0OSabAq",
        "outputId": "cb0b2033-22e0-4e46-ea98-60828ce92d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9606\n",
            "Validation F1 Macro: 0.2854\n",
            "Validation F1 Micro: 0.7028\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('best_model_focal_adamw.pt'))\n",
        "model.eval()\n",
        "# Proceed with evaluation or application of the model\n",
        "with torch.no_grad():\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for images, captions, labels in val_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "        outputs = model(images, captions)\n",
        "        predicted = outputs > 0.5  # Thresholding at 0.5\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.numel()\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "    all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, captions, labels in val_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "        outputs = model(images, captions)\n",
        "\n",
        "        # Apply a threshold to obtain binary predictions\n",
        "        predicted = (outputs > 0.5).type(torch.int)\n",
        "\n",
        "        # Store predictions and labels to calculate F1 score later\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "        all_predictions.append(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for compatibility with scikit-learn\n",
        "all_labels = np.vstack(all_labels)\n",
        "all_predictions = np.vstack(all_predictions)\n",
        "\n",
        "# Calculate F1 scores\n",
        "f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "f1_micro = f1_score(all_labels, all_predictions, average='micro')\n",
        "\n",
        "print(f'Validation F1 Macro: {f1_macro:.4f}')\n",
        "print(f'Validation F1 Micro: {f1_micro:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRqJQeQ0xcii"
      },
      "source": [
        "### Model Training with Focal Loss and Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_NFTGphrxhVB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Train Loss: 0.1034, Val Loss: 0.1033\n",
            "Validation loss decreased (inf --> 0.103344).  Saving model ...\n",
            "Epoch [2/30], Train Loss: 0.1033, Val Loss: 0.1033\n",
            "Validation loss decreased (0.103344 --> 0.103333).  Saving model ...\n",
            "Epoch [3/30], Train Loss: 0.1033, Val Loss: 0.1033\n",
            "Validation loss decreased (0.103333 --> 0.103315).  Saving model ...\n",
            "Epoch [4/30], Train Loss: 0.1033, Val Loss: 0.1033\n",
            "Validation loss decreased (0.103315 --> 0.103295).  Saving model ...\n",
            "Epoch [5/30], Train Loss: 0.1033, Val Loss: 0.1033\n",
            "Validation loss decreased (0.103295 --> 0.103273).  Saving model ...\n",
            "Epoch [6/30], Train Loss: 0.1032, Val Loss: 0.1032\n",
            "Validation loss decreased (0.103273 --> 0.103181).  Saving model ...\n",
            "Epoch [7/30], Train Loss: 0.1031, Val Loss: 0.1031\n",
            "Validation loss decreased (0.103181 --> 0.103141).  Saving model ...\n",
            "Epoch [8/30], Train Loss: 0.1031, Val Loss: 0.1031\n",
            "Validation loss decreased (0.103141 --> 0.103125).  Saving model ...\n",
            "Epoch [9/30], Train Loss: 0.1031, Val Loss: 0.1031\n",
            "Validation loss decreased (0.103125 --> 0.103113).  Saving model ...\n",
            "Epoch [10/30], Train Loss: 0.1031, Val Loss: 0.1031\n",
            "Validation loss decreased (0.103113 --> 0.103097).  Saving model ...\n",
            "Epoch [11/30], Train Loss: 0.1031, Val Loss: 0.1031\n",
            "Validation loss decreased (0.103097 --> 0.103052).  Saving model ...\n",
            "Epoch [12/30], Train Loss: 0.1030, Val Loss: 0.1030\n",
            "Validation loss decreased (0.103052 --> 0.103003).  Saving model ...\n",
            "Epoch [13/30], Train Loss: 0.1030, Val Loss: 0.1030\n",
            "Validation loss decreased (0.103003 --> 0.102981).  Saving model ...\n",
            "Epoch [14/30], Train Loss: 0.1030, Val Loss: 0.1030\n",
            "Validation loss decreased (0.102981 --> 0.102967).  Saving model ...\n",
            "Epoch [15/30], Train Loss: 0.1029, Val Loss: 0.1030\n",
            "Validation loss decreased (0.102967 --> 0.102959).  Saving model ...\n",
            "Epoch [16/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102959 --> 0.102947).  Saving model ...\n",
            "Epoch [17/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102947 --> 0.102938).  Saving model ...\n",
            "Epoch [18/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102938 --> 0.102931).  Saving model ...\n",
            "Epoch [19/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102931 --> 0.102925).  Saving model ...\n",
            "Epoch [20/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102925 --> 0.102918).  Saving model ...\n",
            "Epoch [21/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102918 --> 0.102913).  Saving model ...\n",
            "Epoch [22/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102913 --> 0.102907).  Saving model ...\n",
            "Epoch [23/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102907 --> 0.102900).  Saving model ...\n",
            "Epoch [24/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102900 --> 0.102896).  Saving model ...\n",
            "Epoch [25/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102896 --> 0.102892).  Saving model ...\n",
            "Epoch [26/30], Train Loss: 0.1029, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102892 --> 0.102887).  Saving model ...\n",
            "Epoch [27/30], Train Loss: 0.1028, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102887 --> 0.102883).  Saving model ...\n",
            "Epoch [28/30], Train Loss: 0.1028, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102883 --> 0.102880).  Saving model ...\n",
            "Epoch [29/30], Train Loss: 0.1028, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102880 --> 0.102875).  Saving model ...\n",
            "Epoch [30/30], Train Loss: 0.1028, Val Loss: 0.1029\n",
            "Validation loss decreased (0.102875 --> 0.102873).  Saving model ...\n"
          ]
        }
      ],
      "source": [
        "# Define the criterion with Focal Loss\n",
        "criterion = FocalLossMultiLabel(gamma=0.7)\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "#scheduler = CosineAnnealingLR(optimizer, T_max=100)  # Adjust T_max based on your training epochs\n",
        "# Initialize early stopping object\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, path='best_model_focal_adam.pt')\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, captions, labels in train_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, captions)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Learning rate scheduling step\n",
        "    #scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():  # Use torch.no_grad() during validation\n",
        "        for images, captions, labels in val_dataloader:\n",
        "            images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "            outputs = model(images, captions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    val_loss /= len(val_dataloader)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Early stopping and model checkpointing\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiaKPPCbxnq-"
      },
      "source": [
        "#### Evaluatuin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eOWtXaaoxp2-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9671\n",
            "Validation F1 Macro: 0.4817\n",
            "Validation F1 Micro: 0.7651\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('best_model_focal_adam.pt'))\n",
        "model.eval()\n",
        "# Proceed with evaluation or application of the model\n",
        "with torch.no_grad():\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for images, captions, labels in val_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "        outputs = model(images, captions)\n",
        "        predicted = outputs > 0.5  # Thresholding at 0.5\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.numel()\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "    all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, captions, labels in val_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "        outputs = model(images, captions)\n",
        "\n",
        "        # Apply a threshold to obtain binary predictions\n",
        "        predicted = (outputs > 0.5).type(torch.int)\n",
        "\n",
        "        # Store predictions and labels to calculate F1 score later\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "        all_predictions.append(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for compatibility with scikit-learn\n",
        "all_labels = np.vstack(all_labels)\n",
        "all_predictions = np.vstack(all_predictions)\n",
        "\n",
        "# Calculate F1 scores\n",
        "f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "f1_micro = f1_score(all_labels, all_predictions, average='micro')\n",
        "\n",
        "print(f'Validation F1 Macro: {f1_macro:.4f}')\n",
        "print(f'Validation F1 Micro: {f1_micro:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70RObf97j4h6"
      },
      "source": [
        "### Model Training with Weighted BCE Loss\n",
        "\n",
        "Weighted BCE loss allows you to assign different weights to positive and negative samples, helping to address the class imbalance. You can calculate weights based on the frequency of each class in your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "KP5PbOUKj3wN",
        "outputId": "536b818a-1d83-49ad-96cb-5e5bdcebb217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Train Loss: 0.3241, Val Loss: 0.2945\n",
            "Validation loss decreased (inf --> 0.294454).  Saving model ...\n",
            "Epoch [2/30], Train Loss: 0.2873, Val Loss: 0.2621\n",
            "Validation loss decreased (0.294454 --> 0.262103).  Saving model ...\n",
            "Epoch [3/30], Train Loss: 0.2550, Val Loss: 0.2321\n",
            "Validation loss decreased (0.262103 --> 0.232103).  Saving model ...\n",
            "Epoch [4/30], Train Loss: 0.2246, Val Loss: 0.2040\n",
            "Validation loss decreased (0.232103 --> 0.203976).  Saving model ...\n",
            "Epoch [5/30], Train Loss: 0.1951, Val Loss: 0.1759\n",
            "Validation loss decreased (0.203976 --> 0.175861).  Saving model ...\n",
            "Epoch [6/30], Train Loss: 0.1663, Val Loss: 0.1490\n",
            "Validation loss decreased (0.175861 --> 0.148990).  Saving model ...\n",
            "Epoch [7/30], Train Loss: 0.1392, Val Loss: 0.1244\n",
            "Validation loss decreased (0.148990 --> 0.124385).  Saving model ...\n",
            "Epoch [8/30], Train Loss: 0.1158, Val Loss: 0.1049\n",
            "Validation loss decreased (0.124385 --> 0.104867).  Saving model ...\n",
            "Epoch [9/30], Train Loss: 0.0985, Val Loss: 0.0918\n",
            "Validation loss decreased (0.104867 --> 0.091790).  Saving model ...\n",
            "Epoch [10/30], Train Loss: 0.0877, Val Loss: 0.0845\n",
            "Validation loss decreased (0.091790 --> 0.084544).  Saving model ...\n",
            "Epoch [11/30], Train Loss: 0.0820, Val Loss: 0.0809\n",
            "Validation loss decreased (0.084544 --> 0.080944).  Saving model ...\n",
            "Epoch [12/30], Train Loss: 0.0791, Val Loss: 0.0791\n",
            "Validation loss decreased (0.080944 --> 0.079060).  Saving model ...\n",
            "Epoch [13/30], Train Loss: 0.0775, Val Loss: 0.0779\n",
            "Validation loss decreased (0.079060 --> 0.077876).  Saving model ...\n",
            "Epoch [14/30], Train Loss: 0.0763, Val Loss: 0.0770\n",
            "Validation loss decreased (0.077876 --> 0.076991).  Saving model ...\n",
            "Epoch [15/30], Train Loss: 0.0754, Val Loss: 0.0763\n",
            "Validation loss decreased (0.076991 --> 0.076254).  Saving model ...\n",
            "Epoch [16/30], Train Loss: 0.0746, Val Loss: 0.0758\n",
            "Validation loss decreased (0.076254 --> 0.075752).  Saving model ...\n",
            "Epoch [17/30], Train Loss: 0.0740, Val Loss: 0.0752\n",
            "Validation loss decreased (0.075752 --> 0.075208).  Saving model ...\n",
            "Epoch [18/30], Train Loss: 0.0735, Val Loss: 0.0748\n",
            "Validation loss decreased (0.075208 --> 0.074769).  Saving model ...\n",
            "Epoch [19/30], Train Loss: 0.0730, Val Loss: 0.0745\n",
            "Validation loss decreased (0.074769 --> 0.074460).  Saving model ...\n",
            "Epoch [20/30], Train Loss: 0.0726, Val Loss: 0.0741\n",
            "Validation loss decreased (0.074460 --> 0.074145).  Saving model ...\n",
            "Epoch [21/30], Train Loss: 0.0723, Val Loss: 0.0739\n",
            "Validation loss decreased (0.074145 --> 0.073922).  Saving model ...\n",
            "Epoch [22/30], Train Loss: 0.0719, Val Loss: 0.0737\n",
            "Validation loss decreased (0.073922 --> 0.073675).  Saving model ...\n",
            "Epoch [23/30], Train Loss: 0.0716, Val Loss: 0.0734\n",
            "Validation loss decreased (0.073675 --> 0.073404).  Saving model ...\n",
            "Epoch [24/30], Train Loss: 0.0714, Val Loss: 0.0732\n",
            "Validation loss decreased (0.073404 --> 0.073222).  Saving model ...\n",
            "Epoch [25/30], Train Loss: 0.0711, Val Loss: 0.0731\n",
            "Validation loss decreased (0.073222 --> 0.073082).  Saving model ...\n",
            "Epoch [26/30], Train Loss: 0.0709, Val Loss: 0.0729\n",
            "Validation loss decreased (0.073082 --> 0.072933).  Saving model ...\n",
            "Epoch [27/30], Train Loss: 0.0707, Val Loss: 0.0727\n",
            "Validation loss decreased (0.072933 --> 0.072736).  Saving model ...\n",
            "Epoch [28/30], Train Loss: 0.0705, Val Loss: 0.0726\n",
            "Validation loss decreased (0.072736 --> 0.072628).  Saving model ...\n",
            "Epoch [29/30], Train Loss: 0.0703, Val Loss: 0.0725\n",
            "Validation loss decreased (0.072628 --> 0.072496).  Saving model ...\n",
            "Epoch [30/30], Train Loss: 0.0701, Val Loss: 0.0725\n",
            "Validation loss decreased (0.072496 --> 0.072469).  Saving model ...\n"
          ]
        }
      ],
      "source": [
        "# Set up the loss function and optimizer\n",
        "criterion = BCELoss() #/focal loss\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "#scheduler = CosineAnnealingLR(optimizer, T_max=100)  # Adjust T_max based on your training epochs\n",
        "\n",
        "# Initialize early stopping object\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, path='best_model_bce_adamw.pt')\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, captions, labels in train_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, captions)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Learning rate scheduling step\n",
        "    #scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():  # Use torch.no_grad() during validation\n",
        "        for images, captions, labels in val_dataloader:\n",
        "            images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "            outputs = model(images, captions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    val_loss /= len(val_dataloader)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Early stopping and model checkpointing\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9DZPElYqAvn"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUT0n__iqDg5",
        "outputId": "b244fb46-492a-4f41-ea26-c87a4da78ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9762\n",
            "Validation F1 Macro: 0.7588\n",
            "Validation F1 Micro: 0.8489\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('best_model_bce_adamw.pt'))\n",
        "model.eval()\n",
        "# Proceed with evaluation or application of the model\n",
        "with torch.no_grad():\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for images, captions, labels in val_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "        outputs = model(images, captions)\n",
        "        predicted = outputs > 0.5  # Thresholding at 0.5\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.numel()\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "    all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, captions, labels in val_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "        outputs = model(images, captions)\n",
        "\n",
        "        # Apply a threshold to obtain binary predictions\n",
        "        predicted = (outputs > 0.5).type(torch.int)\n",
        "\n",
        "        # Store predictions and labels to calculate F1 score later\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "        all_predictions.append(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for compatibility with scikit-learn\n",
        "all_labels = np.vstack(all_labels)\n",
        "all_predictions = np.vstack(all_predictions)\n",
        "\n",
        "# Calculate F1 scores\n",
        "f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "f1_micro = f1_score(all_labels, all_predictions, average='micro')\n",
        "\n",
        "print(f'Validation F1 Macro: {f1_macro:.4f}')\n",
        "print(f'Validation F1 Micro: {f1_micro:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDzTeEDUxL6Q"
      },
      "source": [
        "### Model Training with BCE loss and Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc7xmPdLxQeq",
        "outputId": "8afe7695-e484-49e2-8890-37a02ebb7c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Train Loss: 0.0699, Val Loss: 0.0723\n",
            "Validation loss decreased (inf --> 0.072288).  Saving model ...\n",
            "Epoch [2/30], Train Loss: 0.0698, Val Loss: 0.0723\n",
            "Validation loss decreased (0.072288 --> 0.072251).  Saving model ...\n",
            "Epoch [3/30], Train Loss: 0.0696, Val Loss: 0.0721\n",
            "Validation loss decreased (0.072251 --> 0.072112).  Saving model ...\n",
            "Epoch [4/30], Train Loss: 0.0695, Val Loss: 0.0720\n",
            "Validation loss decreased (0.072112 --> 0.072047).  Saving model ...\n",
            "Epoch [5/30], Train Loss: 0.0693, Val Loss: 0.0720\n",
            "Validation loss decreased (0.072047 --> 0.072000).  Saving model ...\n",
            "Epoch [6/30], Train Loss: 0.0692, Val Loss: 0.0719\n",
            "Validation loss decreased (0.072000 --> 0.071851).  Saving model ...\n",
            "Epoch [7/30], Train Loss: 0.0690, Val Loss: 0.0718\n",
            "Validation loss decreased (0.071851 --> 0.071791).  Saving model ...\n",
            "Epoch [8/30], Train Loss: 0.0689, Val Loss: 0.0717\n",
            "Validation loss decreased (0.071791 --> 0.071712).  Saving model ...\n",
            "Epoch [9/30], Train Loss: 0.0688, Val Loss: 0.0716\n",
            "Validation loss decreased (0.071712 --> 0.071642).  Saving model ...\n",
            "Epoch [10/30], Train Loss: 0.0687, Val Loss: 0.0716\n",
            "Validation loss decreased (0.071642 --> 0.071562).  Saving model ...\n",
            "Epoch [11/30], Train Loss: 0.0686, Val Loss: 0.0716\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [12/30], Train Loss: 0.0685, Val Loss: 0.0715\n",
            "Validation loss decreased (0.071562 --> 0.071488).  Saving model ...\n",
            "Epoch [13/30], Train Loss: 0.0684, Val Loss: 0.0714\n",
            "Validation loss decreased (0.071488 --> 0.071394).  Saving model ...\n",
            "Epoch [14/30], Train Loss: 0.0683, Val Loss: 0.0714\n",
            "Validation loss decreased (0.071394 --> 0.071372).  Saving model ...\n",
            "Epoch [15/30], Train Loss: 0.0682, Val Loss: 0.0713\n",
            "Validation loss decreased (0.071372 --> 0.071317).  Saving model ...\n",
            "Epoch [16/30], Train Loss: 0.0681, Val Loss: 0.0713\n",
            "Validation loss decreased (0.071317 --> 0.071316).  Saving model ...\n",
            "Epoch [17/30], Train Loss: 0.0680, Val Loss: 0.0712\n",
            "Validation loss decreased (0.071316 --> 0.071169).  Saving model ...\n",
            "Epoch [18/30], Train Loss: 0.0679, Val Loss: 0.0712\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [19/30], Train Loss: 0.0678, Val Loss: 0.0711\n",
            "Validation loss decreased (0.071169 --> 0.071148).  Saving model ...\n",
            "Epoch [20/30], Train Loss: 0.0677, Val Loss: 0.0712\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [21/30], Train Loss: 0.0676, Val Loss: 0.0711\n",
            "Validation loss decreased (0.071148 --> 0.071085).  Saving model ...\n",
            "Epoch [22/30], Train Loss: 0.0675, Val Loss: 0.0710\n",
            "Validation loss decreased (0.071085 --> 0.071042).  Saving model ...\n",
            "Epoch [23/30], Train Loss: 0.0675, Val Loss: 0.0709\n",
            "Validation loss decreased (0.071042 --> 0.070949).  Saving model ...\n",
            "Epoch [24/30], Train Loss: 0.0674, Val Loss: 0.0709\n",
            "Validation loss decreased (0.070949 --> 0.070935).  Saving model ...\n",
            "Epoch [25/30], Train Loss: 0.0673, Val Loss: 0.0709\n",
            "Validation loss decreased (0.070935 --> 0.070923).  Saving model ...\n",
            "Epoch [26/30], Train Loss: 0.0672, Val Loss: 0.0709\n",
            "Validation loss decreased (0.070923 --> 0.070866).  Saving model ...\n",
            "Epoch [27/30], Train Loss: 0.0671, Val Loss: 0.0709\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [28/30], Train Loss: 0.0671, Val Loss: 0.0709\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [29/30], Train Loss: 0.0670, Val Loss: 0.0709\n",
            "Validation loss decreased (0.070866 --> 0.070859).  Saving model ...\n",
            "Epoch [30/30], Train Loss: 0.0669, Val Loss: 0.0708\n",
            "Validation loss decreased (0.070859 --> 0.070770).  Saving model ...\n"
          ]
        }
      ],
      "source": [
        "# Set up the loss function and optimizer\n",
        "criterion = BCELoss() #/focal loss\n",
        "optimizer = Adam(model.parameters(), lr=1e-4)\n",
        "#scheduler = CosineAnnealingLR(optimizer, T_max=100)  # Adjust T_max based on your training epochs\n",
        "\n",
        "# Initialize early stopping object\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, path='best_model_bce_adam.pt')\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, captions, labels in train_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, captions)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Learning rate scheduling step\n",
        "    #scheduler.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():  # Use torch.no_grad() during validation\n",
        "        for images, captions, labels in val_dataloader:\n",
        "            images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "            outputs = model(images, captions)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    val_loss /= len(val_dataloader)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Early stopping and model checkpointing\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6BRAxRzynW-"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wQp9UZyWyozt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9766\n",
            "Validation F1 Macro: 0.7657\n",
            "Validation F1 Micro: 0.8517\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('best_model_bce_adam.pt'))\n",
        "model.eval()\n",
        "# Proceed with evaluation or application of the model\n",
        "with torch.no_grad():\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for images, captions, labels in val_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "        outputs = model(images, captions)\n",
        "        predicted = outputs > 0.5  # Thresholding at 0.5\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.numel()\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "    all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, captions, labels in val_dataloader:\n",
        "        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n",
        "        outputs = model(images, captions)\n",
        "\n",
        "        # Apply a threshold to obtain binary predictions\n",
        "        predicted = (outputs > 0.5).type(torch.int)\n",
        "\n",
        "        # Store predictions and labels to calculate F1 score later\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "        all_predictions.append(predicted.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays for compatibility with scikit-learn\n",
        "all_labels = np.vstack(all_labels)\n",
        "all_predictions = np.vstack(all_predictions)\n",
        "\n",
        "# Calculate F1 scores\n",
        "f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "f1_micro = f1_score(all_labels, all_predictions, average='micro')\n",
        "\n",
        "print(f'Validation F1 Macro: {f1_macro:.4f}')\n",
        "print(f'Validation F1 Micro: {f1_micro:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9NgHqxAabAq"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WNaVRW4_abAq"
      },
      "outputs": [],
      "source": [
        "# Extract image paths and captions from the DataFrame\n",
        "test_image_paths = test_df['image_path'].tolist()\n",
        "test_captions = test_df['Caption'].tolist()\n",
        "\n",
        "# Preprocess images and tokenize captions\n",
        "processed_test_images = preprocess_images(test_image_paths)\n",
        "tokenized_test_captions = tokenize_captions(test_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvoZItwMabAq",
        "outputId": "2b489d05-7802-4e53-b72b-937fe0382dec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultiLabelCLIPClassifier(\n",
              "  (clip_model): CLIP(\n",
              "    (visual): VisionTransformer(\n",
              "      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (transformer): Transformer(\n",
              "        (resblocks): Sequential(\n",
              "          (0): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (2): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (3): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (4): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (5): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (6): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (7): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (8): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (9): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (10): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (11): ResidualAttentionBlock(\n",
              "            (attn): MultiheadAttention(\n",
              "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Sequential(\n",
              "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (gelu): QuickGELU()\n",
              "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (token_embedding): Embedding(49408, 512)\n",
              "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=18, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the model\n",
        "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "model = MultiLabelCLIPClassifier(clip_model, 18).to(device)  # 18 is the number of labels\n",
        "# Initialize the MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model_focal_adamw.pt'))\n",
        "model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "jHVgLCl1abAq",
        "outputId": "428431ce-1282-472c-f2eb-b6f0f34a9690"
      },
      "outputs": [
        {
          "ename": "NotFittedError",
          "evalue": "This MultiLabelBinarizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m predictions_np \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Use the inverse transform method of MultiLabelBinarizer to get original labels\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m \u001b[43mmlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_np\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:923\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.inverse_transform\u001b[0;34m(self, yt)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minverse_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, yt):\n\u001b[1;32m    910\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform the given indicator matrix into label sets.\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;124;03m        `classes_[j]` for each `yt[i, j] == 1`.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m yt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_):\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    927\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected indicator for \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m classes, but got \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    928\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_), yt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    929\u001b[0m             )\n\u001b[1;32m    930\u001b[0m         )\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:1461\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This MultiLabelBinarizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ],
      "source": [
        "# Make predictions on the test set\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(processed_test_images, tokenized_test_captions)\n",
        "\n",
        "# Convert output probabilities to binary predictions using a threshold\n",
        "predictions = (test_outputs > 0.5).float()\n",
        "\n",
        "# Convert predictions to binary labels\n",
        "predictions_np = predictions.cpu().numpy()\n",
        "\n",
        "# Use the inverse transform method of MultiLabelBinarizer to get original labels\n",
        "predicted_labels = mlb.inverse_transform(predictions_np)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD91lRoOqoEX"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTpvZNAjabAq"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'predicted_labels' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare the submission DataFrame\u001b[39;00m\n\u001b[1;32m      2\u001b[0m submission_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImageID\u001b[39m\u001b[38;5;124m'\u001b[39m: test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImageID\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, labels)) \u001b[38;5;28;01mfor\u001b[39;00m labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpredicted_labels\u001b[49m]\n\u001b[1;32m      5\u001b[0m })\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Save the DataFrame to a CSV file without the index\u001b[39;00m\n\u001b[1;32m      8\u001b[0m submission_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predicted_labels' is not defined"
          ]
        }
      ],
      "source": [
        "# Prepare the submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    'ImageID': test_df['ImageID'],\n",
        "    'Labels': [' '.join(map(str, labels)) for labels in predicted_labels]\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file without the index\n",
        "submission_df.to_csv('submission.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBDApmBQrY0k"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-KmN6ZTqeEf"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c multi-label-classification-competition-2024 -f submission.csv -m \"490236424_480004000_49038816_new\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 8176112,
          "sourceId": 74788,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
