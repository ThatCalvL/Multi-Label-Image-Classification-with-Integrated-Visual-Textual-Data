{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Housekeeping"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-13T01:39:31.617146Z","iopub.status.busy":"2024-05-13T01:39:31.616536Z","iopub.status.idle":"2024-05-13T01:39:47.524145Z","shell.execute_reply":"2024-05-13T01:39:47.523355Z","shell.execute_reply.started":"2024-05-13T01:39:31.617112Z"},"trusted":true},"outputs":[],"source":["import os\n","import re\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from io import StringIO\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","import clip\n","import torch\n","from torch import nn\n","from torch.optim import Adam\n","from torch.nn import BCELoss\n","from sklearn.metrics import f1_score"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-13T01:39:47.527072Z","iopub.status.busy":"2024-05-13T01:39:47.526313Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Retrieving notices: ...working... done\n","Collecting package metadata (current_repodata.json): done\n","Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n","Collecting package metadata (repodata.json): done\n","Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n","\n","PackagesNotFoundError: The following packages are not available from current channels:\n","\n","  - cudatoolkit=11.0\n","\n","Current channels:\n","\n","  - https://conda.anaconda.org/pytorch/osx-64\n","  - https://conda.anaconda.org/pytorch/noarch\n","  - https://repo.anaconda.com/pkgs/main/osx-64\n","  - https://repo.anaconda.com/pkgs/main/noarch\n","  - https://repo.anaconda.com/pkgs/r/osx-64\n","  - https://repo.anaconda.com/pkgs/r/noarch\n","\n","To search for alternate channels that may provide the conda package you're\n","looking for, navigate to\n","\n","    https://anaconda.org\n","\n","and use the search bar at the top of the page.\n","\n","\n","Collecting ftfy\n","  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n","     |████████████████████████████████| 54 kB 5.1 MB/s            \n","\u001b[?25hRequirement already satisfied: regex in /opt/homebrew/lib/python3.11/site-packages (2023.10.3)\n","Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (4.65.0)\n","Collecting wcwidth<0.3.0,>=0.2.12\n","  Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n","Installing collected packages: wcwidth, ftfy\n","  Attempting uninstall: wcwidth\n","    Found existing installation: wcwidth 0.2.6\n","    Uninstalling wcwidth-0.2.6:\n","      Successfully uninstalled wcwidth-0.2.6\n","Successfully installed ftfy-6.2.0 wcwidth-0.2.13\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /private/var/folders/y2/_3pq15pd08556v211gq9jslr0000gn/T/pip-req-build-y_i0c3k2\n","  Running command git clone --filter=blob:none -q https://github.com/openai/CLIP.git /private/var/folders/y2/_3pq15pd08556v211gq9jslr0000gn/T/pip-req-build-y_i0c3k2\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: ftfy in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (6.2.0)\n","Requirement already satisfied: regex in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (2023.10.3)\n","Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (4.65.0)\n","Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (2.0.1)\n","Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (from clip==1.0) (0.15.2)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/homebrew/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (3.12.4)\n","Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (3.1)\n","Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch->clip==1.0) (4.9.0)\n","Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.28.2)\n","Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.24.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from torchvision->clip==1.0) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2023.5.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.1.0)\n","Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369539 sha256=6fa5b356f5e099a2a130c83a8dcd5abdbc1ebd46ee963e94e2ba40e40db57e4d\n","  Stored in directory: /private/var/folders/y2/_3pq15pd08556v211gq9jslr0000gn/T/pip-ephem-wheel-cache-3mq1ynxe/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}],"source":["!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install kaggle"]},{"cell_type":"markdown","metadata":{},"source":["## Data Prep"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ImageID</th>\n","      <th>Labels</th>\n","      <th>Caption</th>\n","      <th>image_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5444</th>\n","      <td>5444.jpg</td>\n","      <td>7</td>\n","      <td>A yellow train traveling down train tracks in ...</td>\n","      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n","    </tr>\n","    <tr>\n","      <th>16727</th>\n","      <td>16727.jpg</td>\n","      <td>1 3 4</td>\n","      <td>A man in yellow vest riding motorcycle in street.</td>\n","      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n","    </tr>\n","    <tr>\n","      <th>26146</th>\n","      <td>26146.jpg</td>\n","      <td>1</td>\n","      <td>A woman with nice breast holding an upside dow...</td>\n","      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n","    </tr>\n","    <tr>\n","      <th>2183</th>\n","      <td>2183.jpg</td>\n","      <td>1</td>\n","      <td>A woman is skiing down a snowy slope with moun...</td>\n","      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n","    </tr>\n","    <tr>\n","      <th>29199</th>\n","      <td>29199.jpg</td>\n","      <td>11</td>\n","      <td>A fire hydrant on someone's lawn in the grass ...</td>\n","      <td>/Users/sugardady/Documents/USYD/2024S1/COMP532...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         ImageID Labels                                            Caption  \\\n","5444    5444.jpg      7  A yellow train traveling down train tracks in ...   \n","16727  16727.jpg  1 3 4  A man in yellow vest riding motorcycle in street.   \n","26146  26146.jpg      1  A woman with nice breast holding an upside dow...   \n","2183    2183.jpg      1  A woman is skiing down a snowy slope with moun...   \n","29199  29199.jpg     11  A fire hydrant on someone's lawn in the grass ...   \n","\n","                                              image_path  \n","5444   /Users/sugardady/Documents/USYD/2024S1/COMP532...  \n","16727  /Users/sugardady/Documents/USYD/2024S1/COMP532...  \n","26146  /Users/sugardady/Documents/USYD/2024S1/COMP532...  \n","2183   /Users/sugardady/Documents/USYD/2024S1/COMP532...  \n","29199  /Users/sugardady/Documents/USYD/2024S1/COMP532...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["DIR = '/Users/sugardady/Documents/USYD/2024S1/COMP5329/A2_due_17_may/COMP5329S1A2Dataset'\n","TRAIN_FILENAME = os.path.join(DIR, \"train.csv\")\n","TEST_FILENAME = os.path.join(DIR, \"test.csv\")\n","IMAGES_DIR = os.path.join(DIR, \"data\")\n","with open(TRAIN_FILENAME) as file:\n","    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n","    data_df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n","    \n","with open(TEST_FILENAME) as file:\n","    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n","    test_df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n","\n","\n","# Read and preprocess data\n","data_df['image_path'] = data_df['ImageID'].apply(lambda x: os.path.join(IMAGES_DIR, x))\n","test_df['image_path'] = test_df['ImageID'].apply(lambda x: os.path.join(IMAGES_DIR, x))\n","# Split data into training and validation sets\n","train_df, val_df = train_test_split(data_df, test_size=0.2, random_state=5329)\n","train_captions = train_df['Caption'].values\n","val_captions = val_df['Caption'].values\n","train_df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Set the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Load the model\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess_images(image_paths):\n","    images = []\n","    for path in image_paths:\n","        image = Image.open(path).convert(\"RGB\")  # Ensure images are in RGB\n","        image = preprocess(image)  # Apply CLIP's preprocessing\n","        images.append(image.unsqueeze(0))  # Add batch dimension\n","    return torch.cat(images).to(device)  # Concatenate all images into a single tensor\n","\n","def tokenize_captions(captions):\n","    return clip.tokenize(captions).to(device)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Extract image paths and captions from the DataFrame\n","image_paths = train_df['image_path'].tolist()\n","captions = train_df['Caption'].tolist()\n","\n","# Preprocess images and tokenize captions\n","processed_images = preprocess_images(image_paths)\n","tokenized_captions = tokenize_captions(captions)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Labels: ['1' '10' '11' '13' '14' '15' '16' '17' '18' '19' '2' '3' '4' '5' '6' '7'\n"," '8' '9']\n","[[1 0 0 ... 0 0 0]\n"," [1 1 0 ... 0 1 0]\n"," [1 0 0 ... 0 0 0]\n"," ...\n"," [1 0 0 ... 0 0 0]\n"," [1 0 0 ... 0 0 0]\n"," [0 1 0 ... 0 0 0]]\n"]}],"source":["# Assume 'data_df' is your DataFrame and it has a 'Labels' column where labels are space-separated\n","train_df['Labels'] = train_df['Labels'].apply(lambda x: x.split())\n","val_df['Labels'] = val_df['Labels'].apply(lambda x: x.split())\n","# Initialize the MultiLabelBinarizer\n","mlb = MultiLabelBinarizer()\n","\n","# Fit and transform the labels to one-hot encoding\n","train_labels_one_hot = mlb.fit_transform(train_df['Labels'])\n","val_labels_one_hot = mlb.fit_transform(val_df['Labels'])\n","# mlb.classes_ will give you the array of all labels that the binarizer has seen\n","print(\"Labels:\", mlb.classes_)\n","\n","# Example output\n","print(val_labels_one_hot)\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, image_paths, captions, labels, transform, tokenize):\n","        self.image_paths = image_paths\n","        self.captions = captions\n","        self.labels = labels\n","        self.transform = transform\n","        self.tokenize = tokenize\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Image processing\n","        image = Image.open(self.image_paths[idx]).convert('RGB')\n","        image = self.transform(image)\n","\n","        # Caption processing\n","        caption = self.tokenize([self.captions[idx]])\n","\n","        # Labels\n","        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n","\n","        return image, caption.squeeze(0), label\n","# Note: These should be functions that operate on a single item and return a tensor\n","transform = preprocess\n","tokenize = clip.tokenize\n","\n","# Create dataset and dataloader\n","train_ds = CustomDataset(\n","    image_paths=train_df['image_path'].tolist(), \n","    captions=train_df['Caption'].tolist(), \n","    labels=train_labels_one_hot, \n","    transform=transform, \n","    tokenize=tokenize\n",")\n","train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True)\n","\n","val_ds = CustomDataset(\n","    \n","    image_paths=val_df['image_path'].tolist(), \n","    captions=val_df['Caption'].tolist(), \n","    labels=val_labels_one_hot, \n","    transform=transform, \n","    tokenize=tokenize\n",")\n","val_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Images: torch.Size([64, 3, 224, 224])\n","Captions: torch.Size([64, 77])\n","Labels: torch.Size([64, 18])\n"]}],"source":["# Quick check to see if everything is loading correctly\n","for images, captions, labels in train_dataloader:\n","    print('Images:', images.shape)  # Expect: [batch_size, C, H, W]\n","    print('Captions:', captions.shape)  # Expect: [batch_size, L]\n","    print('Labels:', labels.shape)  # Expect: [batch_size, num_labels]\n","    break\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model Definition"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["class MultiLabelCLIPClassifier(nn.Module):\n","    def __init__(self, clip_model, num_labels):\n","        super(MultiLabelCLIPClassifier, self).__init__()\n","        self.clip_model = clip_model\n","        self.fc = nn.Linear(512, num_labels)  # 512 is an example feature size, adjust based on CLIP's output\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, images, captions):\n","        with torch.no_grad():\n","            image_features = self.clip_model.encode_image(images)\n","            text_features = self.clip_model.encode_text(captions)\n","\n","        # Combining features: simple addition or concatenation can be used depending on the task\n","        features = (image_features + text_features) / 2\n","        output = self.fc(features)\n","        output = self.sigmoid(output)  # Sigmoid activation for multi-label classification\n","        return output\n","\n","# Initialize the model\n","clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n","model = MultiLabelCLIPClassifier(clip_model, 18).to(device)  # 18 is the number of labels\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class EarlyStopping:\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","            path (str): Path for the checkpoint to be saved to.\n","                            Default: 'checkpoint.pt'\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = float('inf')\n","        self.delta = delta\n","        self.path = path\n","\n","    def __call__(self, val_loss, model):\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss\n","\n","# Initialize early stopping object\n","early_stopping = EarlyStopping(patience=5, verbose=True, path='best_model.pt')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model Training"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/50], Train Loss: 0.0883, Val Loss: 0.0862\n","Validation loss decreased (inf --> 0.086208).  Saving model ...\n","Epoch [2/50], Train Loss: 0.0857, Val Loss: 0.0841\n","Validation loss decreased (0.086208 --> 0.084130).  Saving model ...\n","Epoch [3/50], Train Loss: 0.0837, Val Loss: 0.0825\n","Validation loss decreased (0.084130 --> 0.082517).  Saving model ...\n","Epoch [4/50], Train Loss: 0.0821, Val Loss: 0.0812\n","Validation loss decreased (0.082517 --> 0.081158).  Saving model ...\n","Epoch [5/50], Train Loss: 0.0808, Val Loss: 0.0802\n","Validation loss decreased (0.081158 --> 0.080238).  Saving model ...\n","Epoch [6/50], Train Loss: 0.0796, Val Loss: 0.0792\n","Validation loss decreased (0.080238 --> 0.079203).  Saving model ...\n","Epoch [7/50], Train Loss: 0.0787, Val Loss: 0.0785\n","Validation loss decreased (0.079203 --> 0.078487).  Saving model ...\n","Epoch [8/50], Train Loss: 0.0779, Val Loss: 0.0778\n","Validation loss decreased (0.078487 --> 0.077816).  Saving model ...\n","Epoch [9/50], Train Loss: 0.0771, Val Loss: 0.0772\n","Validation loss decreased (0.077816 --> 0.077222).  Saving model ...\n","Epoch [10/50], Train Loss: 0.0765, Val Loss: 0.0768\n","Validation loss decreased (0.077222 --> 0.076769).  Saving model ...\n","Epoch [11/50], Train Loss: 0.0759, Val Loss: 0.0763\n","Validation loss decreased (0.076769 --> 0.076262).  Saving model ...\n","Epoch [12/50], Train Loss: 0.0754, Val Loss: 0.0759\n","Validation loss decreased (0.076262 --> 0.075898).  Saving model ...\n","Epoch [13/50], Train Loss: 0.0749, Val Loss: 0.0755\n","Validation loss decreased (0.075898 --> 0.075510).  Saving model ...\n","Epoch [14/50], Train Loss: 0.0745, Val Loss: 0.0753\n","Validation loss decreased (0.075510 --> 0.075269).  Saving model ...\n","Epoch [15/50], Train Loss: 0.0741, Val Loss: 0.0749\n","Validation loss decreased (0.075269 --> 0.074894).  Saving model ...\n","Epoch [16/50], Train Loss: 0.0737, Val Loss: 0.0747\n","Validation loss decreased (0.074894 --> 0.074718).  Saving model ...\n","Epoch [17/50], Train Loss: 0.0733, Val Loss: 0.0744\n","Validation loss decreased (0.074718 --> 0.074384).  Saving model ...\n","Epoch [18/50], Train Loss: 0.0730, Val Loss: 0.0741\n","Validation loss decreased (0.074384 --> 0.074138).  Saving model ...\n","Epoch [19/50], Train Loss: 0.0727, Val Loss: 0.0740\n","Validation loss decreased (0.074138 --> 0.074015).  Saving model ...\n","Epoch [20/50], Train Loss: 0.0724, Val Loss: 0.0737\n","Validation loss decreased (0.074015 --> 0.073732).  Saving model ...\n","Epoch [21/50], Train Loss: 0.0722, Val Loss: 0.0735\n","Validation loss decreased (0.073732 --> 0.073515).  Saving model ...\n","Epoch [22/50], Train Loss: 0.0719, Val Loss: 0.0733\n","Validation loss decreased (0.073515 --> 0.073350).  Saving model ...\n","Epoch [23/50], Train Loss: 0.0717, Val Loss: 0.0733\n","Validation loss decreased (0.073350 --> 0.073260).  Saving model ...\n","Epoch [24/50], Train Loss: 0.0715, Val Loss: 0.0731\n","Validation loss decreased (0.073260 --> 0.073132).  Saving model ...\n","Epoch [25/50], Train Loss: 0.0713, Val Loss: 0.0730\n","Validation loss decreased (0.073132 --> 0.072974).  Saving model ...\n","Epoch [26/50], Train Loss: 0.0711, Val Loss: 0.0728\n","Validation loss decreased (0.072974 --> 0.072838).  Saving model ...\n","Epoch [27/50], Train Loss: 0.0709, Val Loss: 0.0727\n","Validation loss decreased (0.072838 --> 0.072724).  Saving model ...\n","Epoch [28/50], Train Loss: 0.0707, Val Loss: 0.0726\n","Validation loss decreased (0.072724 --> 0.072562).  Saving model ...\n","Epoch [29/50], Train Loss: 0.0705, Val Loss: 0.0724\n","Validation loss decreased (0.072562 --> 0.072437).  Saving model ...\n","Epoch [30/50], Train Loss: 0.0703, Val Loss: 0.0724\n","Validation loss decreased (0.072437 --> 0.072372).  Saving model ...\n","Epoch [31/50], Train Loss: 0.0702, Val Loss: 0.0723\n","Validation loss decreased (0.072372 --> 0.072273).  Saving model ...\n","Epoch [32/50], Train Loss: 0.0700, Val Loss: 0.0722\n","Validation loss decreased (0.072273 --> 0.072186).  Saving model ...\n","Epoch [33/50], Train Loss: 0.0699, Val Loss: 0.0721\n","Validation loss decreased (0.072186 --> 0.072110).  Saving model ...\n","Epoch [34/50], Train Loss: 0.0697, Val Loss: 0.0720\n","Validation loss decreased (0.072110 --> 0.071999).  Saving model ...\n","Epoch [35/50], Train Loss: 0.0696, Val Loss: 0.0719\n","Validation loss decreased (0.071999 --> 0.071941).  Saving model ...\n","Epoch [36/50], Train Loss: 0.0694, Val Loss: 0.0719\n","Validation loss decreased (0.071941 --> 0.071861).  Saving model ...\n","Epoch [37/50], Train Loss: 0.0693, Val Loss: 0.0718\n","Validation loss decreased (0.071861 --> 0.071786).  Saving model ...\n","Epoch [38/50], Train Loss: 0.0692, Val Loss: 0.0717\n","Validation loss decreased (0.071786 --> 0.071715).  Saving model ...\n","Epoch [39/50], Train Loss: 0.0691, Val Loss: 0.0716\n","Validation loss decreased (0.071715 --> 0.071625).  Saving model ...\n","Epoch [40/50], Train Loss: 0.0689, Val Loss: 0.0716\n","Validation loss decreased (0.071625 --> 0.071604).  Saving model ...\n","Epoch [41/50], Train Loss: 0.0688, Val Loss: 0.0715\n","Validation loss decreased (0.071604 --> 0.071531).  Saving model ...\n","Epoch [42/50], Train Loss: 0.0687, Val Loss: 0.0715\n","Validation loss decreased (0.071531 --> 0.071482).  Saving model ...\n","Epoch [43/50], Train Loss: 0.0686, Val Loss: 0.0714\n","Validation loss decreased (0.071482 --> 0.071420).  Saving model ...\n","Epoch [44/50], Train Loss: 0.0685, Val Loss: 0.0714\n","Validation loss decreased (0.071420 --> 0.071365).  Saving model ...\n","Epoch [45/50], Train Loss: 0.0684, Val Loss: 0.0713\n","Validation loss decreased (0.071365 --> 0.071275).  Saving model ...\n","Epoch [46/50], Train Loss: 0.0683, Val Loss: 0.0713\n","EarlyStopping counter: 1 out of 5\n","Epoch [47/50], Train Loss: 0.0682, Val Loss: 0.0712\n","Validation loss decreased (0.071275 --> 0.071240).  Saving model ...\n","Epoch [48/50], Train Loss: 0.0681, Val Loss: 0.0712\n","Validation loss decreased (0.071240 --> 0.071231).  Saving model ...\n","Epoch [49/50], Train Loss: 0.0680, Val Loss: 0.0712\n","Validation loss decreased (0.071231 --> 0.071167).  Saving model ...\n","Epoch [50/50], Train Loss: 0.0679, Val Loss: 0.0711\n","Validation loss decreased (0.071167 --> 0.071104).  Saving model ...\n"]}],"source":["# Set up the loss function and optimizer\n","criterion = BCELoss() #/focal loss\n","optimizer = Adam(model.parameters(), lr=1e-4)\n","\n","num_epochs = 50  # Set the number of epochs\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for images, captions, labels in train_dataloader:\n","        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n","        \n","        # Forward pass\n","        outputs = model(images, captions)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss.item()\n","\n","    # Validation phase\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for images, captions, labels in val_dataloader:\n","            images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n","            outputs = model(images, captions)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","    # Calculate average losses\n","    train_loss = total_loss / len(train_dataloader)\n","    val_loss /= len(val_dataloader)\n","    \n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","    \n","    # Early stopping and model checkpointing\n","    early_stopping(val_loss, model)\n","    if early_stopping.early_stop:\n","        print(\"Early stopping\")\n","        break\n"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.9763\n","Validation F1 Macro: 0.7632\n","Validation F1 Micro: 0.8493\n"]}],"source":["model.load_state_dict(torch.load('best_model.pt'))\n","model.eval()\n","# Proceed with evaluation or application of the model\n","with torch.no_grad():\n","    correct_predictions = 0\n","    total_predictions = 0\n","    for images, captions, labels in val_dataloader:\n","        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n","        outputs = model(images, captions)\n","        predicted = outputs > 0.5  # Thresholding at 0.5\n","        correct_predictions += (predicted == labels).sum().item()\n","        total_predictions += labels.numel()\n","\n","    accuracy = correct_predictions / total_predictions\n","    print(f'Validation Accuracy: {accuracy:.4f}')\n","    all_labels = []\n","all_predictions = []\n","\n","with torch.no_grad():\n","    for images, captions, labels in val_dataloader:\n","        images, captions, labels = images.to(device), captions.to(device), labels.to(device)\n","        outputs = model(images, captions)\n","        \n","        # Apply a threshold to obtain binary predictions\n","        predicted = (outputs > 0.5).type(torch.int)\n","        \n","        # Store predictions and labels to calculate F1 score later\n","        all_labels.append(labels.cpu().numpy())\n","        all_predictions.append(predicted.cpu().numpy())\n","\n","# Convert lists to numpy arrays for compatibility with scikit-learn\n","all_labels = np.vstack(all_labels)\n","all_predictions = np.vstack(all_predictions)\n","\n","# Calculate F1 scores\n","f1_macro = f1_score(all_labels, all_predictions, average='macro')\n","f1_micro = f1_score(all_labels, all_predictions, average='micro')\n","\n","print(f'Validation F1 Macro: {f1_macro:.4f}')\n","print(f'Validation F1 Micro: {f1_micro:.4f}')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Test"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# Extract image paths and captions from the DataFrame\n","test_image_paths = test_df['image_path'].tolist()\n","test_captions = test_df['Caption'].tolist()\n","\n","# Preprocess images and tokenize captions\n","processed_test_images = preprocess_images(test_image_paths)\n","tokenized_test_captions = tokenize_captions(test_captions)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["MultiLabelCLIPClassifier(\n","  (clip_model): CLIP(\n","    (visual): VisionTransformer(\n","      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n","      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (transformer): Transformer(\n","        (resblocks): Sequential(\n","          (0): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (1): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (2): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (3): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (4): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (5): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (6): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (7): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (8): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (9): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (10): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (11): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (transformer): Transformer(\n","      (resblocks): Sequential(\n","        (0): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (6): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (7): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (8): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (9): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (10): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (11): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (token_embedding): Embedding(49408, 512)\n","    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (fc): Linear(in_features=512, out_features=18, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize the model\n","clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n","model = MultiLabelCLIPClassifier(clip_model, 18).to(device)  # 18 is the number of labels\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model.pt'))\n","model.eval()  # Set the model to evaluation mode"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# Make predictions on the test set\n","with torch.no_grad():\n","    test_outputs = model(processed_test_images, tokenized_test_captions)\n","\n","# Convert output probabilities to binary predictions using a threshold\n","predictions = (test_outputs > 0.5).float()\n","\n","# Convert predictions to binary labels\n","predictions_np = predictions.cpu().numpy()\n","\n","# Use the inverse transform method of MultiLabelBinarizer to get original labels\n","predicted_labels = mlb.inverse_transform(predictions_np)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Submission"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Prepare the submission DataFrame\n","submission_df = pd.DataFrame({\n","    'ImageID': test_df['ImageID'],\n","    'Labels': [' '.join(map(str, labels)) for labels in predicted_labels]\n","})\n","\n","# Save the DataFrame to a CSV file without the index\n","submission_df.to_csv('submission.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!kaggle competitions submit -c multi-label-classification-competition-2024 -f submission.csv -m \"490236424_480004000_490388169\""]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8176112,"sourceId":74788,"sourceType":"competition"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
